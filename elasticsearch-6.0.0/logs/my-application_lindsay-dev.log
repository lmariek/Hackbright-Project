[2017-12-01T02:21:26,419][INFO ][o.e.c.m.MetaDataMappingService] [node-1_lindsay_01-dev] [documents/GLU7-rz3QqaIUD33o8r9Qw] update_mapping [complaint]
[2017-12-01T03:18:46,297][INFO ][o.e.n.Node               ] [node-1_lindsay_01-dev] stopping ...
[2017-12-01T03:18:46,337][INFO ][o.e.n.Node               ] [node-1_lindsay_01-dev] stopped
[2017-12-01T03:18:46,337][INFO ][o.e.n.Node               ] [node-1_lindsay_01-dev] closing ...
[2017-12-01T03:18:46,358][INFO ][o.e.n.Node               ] [node-1_lindsay_01-dev] closed
[2017-12-01T03:21:24,617][WARN ][o.e.b.JNANatives         ] Unable to lock JVM Memory: error=12, reason=Cannot allocate memory
[2017-12-01T03:21:24,646][WARN ][o.e.b.JNANatives         ] This can result in part of the JVM being swapped out.
[2017-12-01T03:21:24,647][WARN ][o.e.b.JNANatives         ] Increase RLIMIT_MEMLOCK, soft limit: 65536, hard limit: 65536
[2017-12-01T03:21:24,647][WARN ][o.e.b.JNANatives         ] These can be adjusted by modifying /etc/security/limits.conf, for example: 
	# allow user 'vagrant' mlockall
	vagrant soft memlock unlimited
	vagrant hard memlock unlimited
[2017-12-01T03:21:24,647][WARN ][o.e.b.JNANatives         ] If you are logged in interactively, you will have to re-login for the new limits to take effect.
[2017-12-01T03:21:25,824][INFO ][o.e.n.Node               ] [node-1_lindsay_01-dev] initializing ...
[2017-12-01T03:21:26,095][INFO ][o.e.e.NodeEnvironment    ] [node-1_lindsay_01-dev] using [1] data paths, mounts [[/home/vagrant/src (home_vagrant_src)]], net usable_space [292.3gb], net total_space [464.7gb], types [vboxsf]
[2017-12-01T03:21:26,097][INFO ][o.e.e.NodeEnvironment    ] [node-1_lindsay_01-dev] heap size [1015.6mb], compressed ordinary object pointers [true]
[2017-12-01T03:21:26,581][INFO ][o.e.n.Node               ] [node-1_lindsay_01-dev] node name [node-1_lindsay_01-dev], node ID [K6il1mL8SgO_OsMykcPBjA]
[2017-12-01T03:21:26,583][INFO ][o.e.n.Node               ] [node-1_lindsay_01-dev] version[6.0.0], pid[29570], build[8f0685b/2017-11-10T18:41:22.859Z], OS[Linux/4.4.0-92-generic/amd64], JVM[Oracle Corporation/Java HotSpot(TM) 64-Bit Server VM/1.8.0_151/25.151-b12]
[2017-12-01T03:21:26,583][INFO ][o.e.n.Node               ] [node-1_lindsay_01-dev] JVM arguments [-Xms1g, -Xmx1g, -XX:+UseConcMarkSweepGC, -XX:CMSInitiatingOccupancyFraction=75, -XX:+UseCMSInitiatingOccupancyOnly, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -XX:+HeapDumpOnOutOfMemoryError, -Des.path.home=/home/vagrant/src/Projects/ocr/elasticsearch-6.0.0, -Des.path.conf=/home/vagrant/src/Projects/ocr/elasticsearch-6.0.0/config]
[2017-12-01T03:21:33,348][INFO ][o.e.p.PluginsService     ] [node-1_lindsay_01-dev] loaded module [aggs-matrix-stats]
[2017-12-01T03:21:33,350][INFO ][o.e.p.PluginsService     ] [node-1_lindsay_01-dev] loaded module [analysis-common]
[2017-12-01T03:21:33,350][INFO ][o.e.p.PluginsService     ] [node-1_lindsay_01-dev] loaded module [ingest-common]
[2017-12-01T03:21:33,351][INFO ][o.e.p.PluginsService     ] [node-1_lindsay_01-dev] loaded module [lang-expression]
[2017-12-01T03:21:33,352][INFO ][o.e.p.PluginsService     ] [node-1_lindsay_01-dev] loaded module [lang-mustache]
[2017-12-01T03:21:33,352][INFO ][o.e.p.PluginsService     ] [node-1_lindsay_01-dev] loaded module [lang-painless]
[2017-12-01T03:21:33,353][INFO ][o.e.p.PluginsService     ] [node-1_lindsay_01-dev] loaded module [parent-join]
[2017-12-01T03:21:33,354][INFO ][o.e.p.PluginsService     ] [node-1_lindsay_01-dev] loaded module [percolator]
[2017-12-01T03:21:33,354][INFO ][o.e.p.PluginsService     ] [node-1_lindsay_01-dev] loaded module [reindex]
[2017-12-01T03:21:33,356][INFO ][o.e.p.PluginsService     ] [node-1_lindsay_01-dev] loaded module [repository-url]
[2017-12-01T03:21:33,357][INFO ][o.e.p.PluginsService     ] [node-1_lindsay_01-dev] loaded module [transport-netty4]
[2017-12-01T03:21:33,358][INFO ][o.e.p.PluginsService     ] [node-1_lindsay_01-dev] loaded module [tribe]
[2017-12-01T03:21:33,363][INFO ][o.e.p.PluginsService     ] [node-1_lindsay_01-dev] loaded plugin [ingest-attachment]
[2017-12-01T03:21:37,680][INFO ][o.e.d.DiscoveryModule    ] [node-1_lindsay_01-dev] using discovery type [zen]
[2017-12-01T03:21:39,363][INFO ][o.e.n.Node               ] [node-1_lindsay_01-dev] initialized
[2017-12-01T03:21:39,364][INFO ][o.e.n.Node               ] [node-1_lindsay_01-dev] starting ...
[2017-12-01T03:21:39,939][INFO ][o.e.t.TransportService   ] [node-1_lindsay_01-dev] publish_address {127.0.0.1:9300}, bound_addresses {[::1]:9300}, {127.0.0.1:9300}
[2017-12-01T03:21:40,044][WARN ][o.e.b.BootstrapChecks    ] [node-1_lindsay_01-dev] memory locking requested for elasticsearch process but memory is not locked
[2017-12-01T03:21:40,045][WARN ][o.e.b.BootstrapChecks    ] [node-1_lindsay_01-dev] max number of threads [3811] for user [vagrant] is too low, increase to at least [4096]
[2017-12-01T03:21:40,047][WARN ][o.e.b.BootstrapChecks    ] [node-1_lindsay_01-dev] max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]
[2017-12-01T03:21:43,190][INFO ][o.e.c.s.MasterService    ] [node-1_lindsay_01-dev] zen-disco-elected-as-master ([0] nodes joined), reason: new_master {node-1_lindsay_01-dev}{K6il1mL8SgO_OsMykcPBjA}{KSazw2-DQ7SFZDvfitIwlQ}{127.0.0.1}{127.0.0.1:9300}
[2017-12-01T03:21:43,200][INFO ][o.e.c.s.ClusterApplierService] [node-1_lindsay_01-dev] new_master {node-1_lindsay_01-dev}{K6il1mL8SgO_OsMykcPBjA}{KSazw2-DQ7SFZDvfitIwlQ}{127.0.0.1}{127.0.0.1:9300}, reason: apply cluster state (from master [master {node-1_lindsay_01-dev}{K6il1mL8SgO_OsMykcPBjA}{KSazw2-DQ7SFZDvfitIwlQ}{127.0.0.1}{127.0.0.1:9300} committed version [1] source [zen-disco-elected-as-master ([0] nodes joined)]])
[2017-12-01T03:21:43,239][INFO ][o.e.h.n.Netty4HttpServerTransport] [node-1_lindsay_01-dev] publish_address {127.0.0.1:9200}, bound_addresses {[::1]:9200}, {127.0.0.1:9200}
[2017-12-01T03:21:43,242][INFO ][o.e.n.Node               ] [node-1_lindsay_01-dev] started
[2017-12-01T03:21:43,740][INFO ][o.e.g.GatewayService     ] [node-1_lindsay_01-dev] recovered [1] indices into cluster_state
[2017-12-01T03:21:45,197][INFO ][o.e.c.r.a.AllocationService] [node-1_lindsay_01-dev] Cluster health status changed from [RED] to [YELLOW] (reason: [shards started [[documents][3]] ...]).
[2017-12-01T03:26:48,543][INFO ][o.e.m.j.JvmGcMonitorService] [node-1_lindsay_01-dev] [gc][309] overhead, spent [277ms] collecting in the last [1s]
